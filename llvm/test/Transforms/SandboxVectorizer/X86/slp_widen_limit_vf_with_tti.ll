; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 2
; RUN: opt -passes=sandbox-vectorizer -sbvec-cost-threshold=-9999 -mtriple=x86_64-- -mattr=+sse4.1 %s -S | FileCheck %s --check-prefix=VECTOR-SSE4
; RUN: opt -passes=sandbox-vectorizer -sbvec-cost-threshold=-9999 -mtriple=x86_64-- -mattr=+avx %s -S | FileCheck %s --check-prefix=VECTOR-AVX
; RUN: opt -passes=sandbox-vectorizer -sbvec-cost-threshold=-9999 -mtriple=x86_64-- -mattr=+avx512vl %s -S | FileCheck %s --check-prefix=VECTOR-AVX512VL
; RUN: opt -passes=sandbox-vectorizer -sbvec-cost-threshold=99999 -mtriple=x86_64-- -mattr=+avx512vl %s -S | FileCheck %s --check-prefix=SCALAR

define void @slp_widen_limit_vf_with_tti(ptr %ptr) {
; VECTOR-SSE4-LABEL: define void @slp_widen_limit_vf_with_tti
; VECTOR-SSE4-SAME: (ptr [[PTR:%.*]]) #[[ATTR0:[0-9]+]] {
; VECTOR-SSE4-NEXT:    [[PTR0:%.*]] = getelementptr double, ptr [[PTR]], i32 0
; VECTOR-SSE4-NEXT:    [[PTR2:%.*]] = getelementptr double, ptr [[PTR]], i32 2
; VECTOR-SSE4-NEXT:    [[PTR4:%.*]] = getelementptr double, ptr [[PTR]], i32 4
; VECTOR-SSE4-NEXT:    [[PTR6:%.*]] = getelementptr double, ptr [[PTR]], i32 6
; VECTOR-SSE4-NEXT:    [[VECL:%.*]] = load <2 x double>, ptr [[PTR0]], align 8
; VECTOR-SSE4-NEXT:    [[VECL1:%.*]] = load <2 x double>, ptr [[PTR2]], align 8
; VECTOR-SSE4-NEXT:    [[VECL2:%.*]] = load <2 x double>, ptr [[PTR4]], align 8
; VECTOR-SSE4-NEXT:    [[VECL3:%.*]] = load <2 x double>, ptr [[PTR6]], align 8
; VECTOR-SSE4-NEXT:    store <2 x double> [[VECL]], ptr [[PTR0]], align 8
; VECTOR-SSE4-NEXT:    store <2 x double> [[VECL1]], ptr [[PTR2]], align 8
; VECTOR-SSE4-NEXT:    store <2 x double> [[VECL2]], ptr [[PTR4]], align 8
; VECTOR-SSE4-NEXT:    store <2 x double> [[VECL3]], ptr [[PTR6]], align 8
; VECTOR-SSE4-NEXT:    ret void
;
; VECTOR-AVX-LABEL: define void @slp_widen_limit_vf_with_tti
; VECTOR-AVX-SAME: (ptr [[PTR:%.*]]) #[[ATTR0:[0-9]+]] {
; VECTOR-AVX-NEXT:    [[PTR0:%.*]] = getelementptr double, ptr [[PTR]], i32 0
; VECTOR-AVX-NEXT:    [[PTR4:%.*]] = getelementptr double, ptr [[PTR]], i32 4
; VECTOR-AVX-NEXT:    [[VECL:%.*]] = load <4 x double>, ptr [[PTR0]], align 8
; VECTOR-AVX-NEXT:    [[VECL1:%.*]] = load <4 x double>, ptr [[PTR4]], align 8
; VECTOR-AVX-NEXT:    store <4 x double> [[VECL]], ptr [[PTR0]], align 8
; VECTOR-AVX-NEXT:    store <4 x double> [[VECL1]], ptr [[PTR4]], align 8
; VECTOR-AVX-NEXT:    ret void
;
; VECTOR-AVX512VL-LABEL: define void @slp_widen_limit_vf_with_tti
; VECTOR-AVX512VL-SAME: (ptr [[PTR:%.*]]) #[[ATTR0:[0-9]+]] {
; VECTOR-AVX512VL-NEXT:    [[PTR0:%.*]] = getelementptr double, ptr [[PTR]], i32 0
; VECTOR-AVX512VL-NEXT:    [[VECL:%.*]] = load <8 x double>, ptr [[PTR0]], align 8
; VECTOR-AVX512VL-NEXT:    store <8 x double> [[VECL]], ptr [[PTR0]], align 8
; VECTOR-AVX512VL-NEXT:    ret void
;
; SCALAR-LABEL: define void @slp_widen_limit_vf_with_tti
; SCALAR-SAME: (ptr [[PTR:%.*]]) #[[ATTR0:[0-9]+]] {
; SCALAR-NEXT:    [[PTR0:%.*]] = getelementptr double, ptr [[PTR]], i32 0
; SCALAR-NEXT:    [[PTR1:%.*]] = getelementptr double, ptr [[PTR]], i32 1
; SCALAR-NEXT:    [[PTR2:%.*]] = getelementptr double, ptr [[PTR]], i32 2
; SCALAR-NEXT:    [[PTR3:%.*]] = getelementptr double, ptr [[PTR]], i32 3
; SCALAR-NEXT:    [[PTR4:%.*]] = getelementptr double, ptr [[PTR]], i32 4
; SCALAR-NEXT:    [[PTR5:%.*]] = getelementptr double, ptr [[PTR]], i32 5
; SCALAR-NEXT:    [[PTR6:%.*]] = getelementptr double, ptr [[PTR]], i32 6
; SCALAR-NEXT:    [[PTR7:%.*]] = getelementptr double, ptr [[PTR]], i32 7
; SCALAR-NEXT:    [[LD0:%.*]] = load double, ptr [[PTR0]], align 8
; SCALAR-NEXT:    [[LD1:%.*]] = load double, ptr [[PTR1]], align 8
; SCALAR-NEXT:    [[LD2:%.*]] = load double, ptr [[PTR2]], align 8
; SCALAR-NEXT:    [[LD3:%.*]] = load double, ptr [[PTR3]], align 8
; SCALAR-NEXT:    [[LD4:%.*]] = load double, ptr [[PTR4]], align 8
; SCALAR-NEXT:    [[LD5:%.*]] = load double, ptr [[PTR5]], align 8
; SCALAR-NEXT:    [[LD6:%.*]] = load double, ptr [[PTR6]], align 8
; SCALAR-NEXT:    [[LD7:%.*]] = load double, ptr [[PTR7]], align 8
; SCALAR-NEXT:    store double [[LD0]], ptr [[PTR0]], align 8
; SCALAR-NEXT:    store double [[LD1]], ptr [[PTR1]], align 8
; SCALAR-NEXT:    store double [[LD2]], ptr [[PTR2]], align 8
; SCALAR-NEXT:    store double [[LD3]], ptr [[PTR3]], align 8
; SCALAR-NEXT:    store double [[LD4]], ptr [[PTR4]], align 8
; SCALAR-NEXT:    store double [[LD5]], ptr [[PTR5]], align 8
; SCALAR-NEXT:    store double [[LD6]], ptr [[PTR6]], align 8
; SCALAR-NEXT:    store double [[LD7]], ptr [[PTR7]], align 8
; SCALAR-NEXT:    ret void
;
  %ptr0 = getelementptr double, ptr %ptr, i32 0
  %ptr1 = getelementptr double, ptr %ptr, i32 1
  %ptr2 = getelementptr double, ptr %ptr, i32 2
  %ptr3 = getelementptr double, ptr %ptr, i32 3
  %ptr4 = getelementptr double, ptr %ptr, i32 4
  %ptr5 = getelementptr double, ptr %ptr, i32 5
  %ptr6 = getelementptr double, ptr %ptr, i32 6
  %ptr7 = getelementptr double, ptr %ptr, i32 7
  %ld0 = load double, ptr %ptr0
  %ld1 = load double, ptr %ptr1
  %ld2 = load double, ptr %ptr2
  %ld3 = load double, ptr %ptr3
  %ld4 = load double, ptr %ptr4
  %ld5 = load double, ptr %ptr5
  %ld6 = load double, ptr %ptr6
  %ld7 = load double, ptr %ptr7
  store double %ld0, ptr %ptr0
  store double %ld1, ptr %ptr1
  store double %ld2, ptr %ptr2
  store double %ld3, ptr %ptr3
  store double %ld4, ptr %ptr4
  store double %ld5, ptr %ptr5
  store double %ld6, ptr %ptr6
  store double %ld7, ptr %ptr7
  ret void
}
